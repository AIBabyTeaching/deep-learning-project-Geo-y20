{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as im\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# files i wrote\n",
    "import CropAndNormalize as cAn\n",
    "import LineExtraction as lN\n",
    "\n",
    "########################################Letters To Train\n",
    "letters = ['a','b','c','d','e','f','g','h','i',              # Letters to learn \n",
    "           'j','k','l','m','n','o','p','q','r',\n",
    "           's','t','u','v','w','x','y','z']\n",
    "\n",
    "##########################################################\n",
    "\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#------------------------------Weights initialization------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "  \n",
    "def initializeWeights(width,height,numberOfHiddenNeurons):\n",
    "    Wi_h = np.random.random(size=(numberOfHiddenNeurons,height,width))-0.5  # input to hidden layer weights\n",
    "    Wh_o = np.random.random(size=(26,numberOfHiddenNeurons))-0.5            # hidden to output weights\n",
    "    Bh   = np.random.random(numberOfHiddenNeurons) - 0.5                    # hidden layer biases\n",
    "    Bo   = np.random.random(26) - 0.5                                       # output layer biases    \n",
    "    \n",
    "    return Wi_h, Wh_o, Bh, Bo\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#------------------------------Threshold Function----------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "def logistic(summation):                                                    # sigmoid function\n",
    "       out = 1 / (1 + np.exp(-summation))\n",
    "       return out\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#------------------------------FEED FORWARD THROUGH NETWORK------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "       \n",
    "def feedForward(normalized, Wi_h, Wh_o, Bh, Bo):                            # feed forward through net\n",
    "    \n",
    "    n_h                                  = 0\n",
    "    [numberOfHiddenNeurons,height,width] = Wi_h.shape\n",
    "    #-----------------------------feed Forward, From input layer to hidden\n",
    "    outputOfHiddenNeurons                = []\n",
    "    netInputForHiddenNeurons             = []\n",
    "    \n",
    "    for hiddenNeuron in range (0,numberOfHiddenNeurons):                 # forward pass from input to hidden\n",
    "        \n",
    "        for i in range (0,height):                                       # calculating net activation input\n",
    "            for j in range (0,width):                           \n",
    "                WxP = Wi_h[hiddenNeuron,i,j] * normalized[i,j]           # Weight X Input \n",
    "                n_h   = n_h + WxP                                        # The overall sum of W's X P's\n",
    "               \n",
    "        n_h                   = n_h + Bh[hiddenNeuron]                   # total input = WP+Bias\n",
    "        outputOfHiddenNeurons.append(logistic(n_h))                      # calculate and save hidden neurons output\n",
    "        netInputForHiddenNeurons.append(n_h)                             # save total net input\n",
    "        n_h                   = 0                                        # reset n\n",
    "\n",
    "    #---------------------------feed forward, from hidden to output\n",
    "    outHiddenXweightsH_O      = outputOfHiddenNeurons * Wh_o             # out of hidden layer multiplied by weights from hidden to output layer\n",
    "                                                                         # this is a 26 X 10 matrix, each row contains the weights connecting hidden neurons to specific out neuron.\n",
    "    netInputForOutNeurons     = np.sum(outHiddenXweightsH_O, axis= 1)    # sum of all (Wh_o weights X hidden neuron outputs), each row is the total input for each output neuron\n",
    "    outputOfOutNeurons        = []\n",
    "    \n",
    "    for outputNeuron in range(0,26):\n",
    "                                                                         # Find and Calculate output of out neurons\n",
    "        totalInputForNeuron   = netInputForOutNeurons[outputNeuron] + Bo[outputNeuron]   # the input to the kth output neuron\n",
    "        outputOfOutNeurons.append(logistic(totalInputForNeuron))         # get the output and save it\n",
    "\n",
    "    return outputOfOutNeurons, outputOfHiddenNeurons                     # return outputs\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#------------------------------Calculate Error At Output Neurons-------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "   \n",
    "\n",
    "def calculateErrorAtOutput(outputOfOutNeurons, targetOutput):\n",
    "    outputError =[]\n",
    "    for outputNeuron in range(0,26):\n",
    "                                                                         # Calculating the error at the output\n",
    "        outputNeuronError     = outputOfOutNeurons[outputNeuron] - targetOutput[outputNeuron] # error = out - target\n",
    "        outputError.append(outputNeuronError)                            # save error for all outputs\n",
    "    return outputError                                                   # return error\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#------------------------------BACK PROPAGATE AND ADJUST WEIGHTS ------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "   \n",
    "def backPropagate(Wi_h, Wh_o, Bh, Bo, normalized, outputError, outputOfOutNeurons, outputOfHiddenNeurons, learningRate, momentum):\n",
    "\n",
    "    oldWh_o    = np.array(Wh_o[:,:])                                     # save old weights for b.prop from hidden to input  \n",
    "    oldWi_h    = np.array(Wi_h[:,:]) \n",
    "   \n",
    "    [numberOfHiddenNeurons,height,width] = Wi_h.shape\n",
    "    #---------------------------------------Back Propagating from output to hidden and adjusting weights\n",
    "    for outputNeuron in range(0,26):\n",
    "        for hiddenNeuron in range(0,numberOfHiddenNeurons):\n",
    "            # calculating the adjustment which is learning rate * error at current output neuron * sigmoid derivative *  output of current hidden neuron)\n",
    "            adjustment                       = (learningRate * outputError[outputNeuron] * outputOfOutNeurons[outputNeuron] * (1 - outputOfOutNeurons[outputNeuron]) * outputOfHiddenNeurons[hiddenNeuron])\n",
    "            Wh_o[outputNeuron, hiddenNeuron] = (momentum * Wh_o[outputNeuron, hiddenNeuron]) - adjustment # adjusting weights per this formula, Wnew = momentum* Wold - adjustment\n",
    "\n",
    "    #---------------------------------------Back Propagating from hidden to input and adjusting weights\n",
    "    for hiddenNeuron in range(0,numberOfHiddenNeurons):\n",
    "        deltaTotalError_hiddenNeuron = 0\n",
    "        for outputNeuron in range(0,26):\n",
    "            # Calculate delta error at each output neuron with respect to current hidden neuron\n",
    "            deltaErrorOutputNeuron_hiddenNeuron = outputError[outputNeuron] *  outputOfOutNeurons[outputNeuron] * (1-outputOfOutNeurons[outputNeuron]) * oldWh_o[outputNeuron,hiddenNeuron]\n",
    "            deltaTotalError_hiddenNeuron = deltaTotalError_hiddenNeuron + deltaErrorOutputNeuron_hiddenNeuron # delta total error with respect to current hidden neuron\n",
    "\n",
    "        # loop over all input weights connecting to current hidden neuron\n",
    "        for i in range (0,height):\n",
    "            for j in range (0,width):\n",
    "                # delta Total Error with respect to weight to be adjusted. this weight is connecting input to current hidden layer\n",
    "                deltaTotalError_inputTohiddenNeuronWeight = deltaTotalError_hiddenNeuron * outputOfHiddenNeurons[hiddenNeuron] *(1 - outputOfHiddenNeurons[hiddenNeuron]) * normalized[i,j]\n",
    "                Wi_h[hiddenNeuron,i,j] = (momentum * Wi_h[hiddenNeuron,i,j]) - (learningRate * deltaTotalError_inputTohiddenNeuronWeight)\n",
    "\n",
    "    return Wi_h, Wh_o\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#------------------------------TRAIN NETWORK --------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "   \n",
    "def trainNet(Wi_h, Wh_o, Bh, Bo,height,width,numberOfTrainingSamples,learningRate,momentum,targetError):\n",
    "    iteration  = 0\n",
    "    totalError = 1\n",
    "    errorList  = []     # to save all total error generated\n",
    "    y_axis     = []     # for plotting the error minimization at the end\n",
    "        \n",
    "    while totalError > targetError:                                            # loop until criteria is met\n",
    "\n",
    "        for letterToTrain in range(0,26):                                      # loop for all letters to be trained \n",
    "            targetOutput                = np.zeros(26)                         # target output is all zeros\n",
    "            targetOutput[letterToTrain] = 1                                    # except the one to be trained\n",
    "            \n",
    "            for n in range (0,numberOfTrainingSamples):                        # number of training samples \n",
    "\n",
    "                #---------------Cropping and Normalizing the image to have a uniform input to the ANN\n",
    "                trainingSample      = 'samples/%s%d.png' %(letters[letterToTrain],n) # training sample image file name\n",
    "\n",
    "                character_in        = im.open(trainingSample)                  # load Image\n",
    "                blackAndWhite       = cAn.convertToBW(character_in)            # Convert to BW, 1->black\n",
    "                toggledBW           = cAn.toggleOnesAndZeros(blackAndWhite)\n",
    "                croppedBW           = cAn.crop(toggledBW)                      # Crop Image to get character only\n",
    "                normalized          = cAn.normalize(croppedBW,width,height)    # Normalize (resize)\n",
    "               #----------------------- end of pre processing phase\n",
    "                \n",
    "                outputOfOutNeurons, outputOfHiddenNeurons = feedForward(normalized, Wi_h, Wh_o, Bh, Bo) # feed forward\n",
    "                outputError         = calculateErrorAtOutput(outputOfOutNeurons, targetOutput)          # calculate error at output neurons\n",
    "                Wi_h, Wh_o          = backPropagate(Wi_h, Wh_o, Bh, Bo, normalized, outputError, outputOfOutNeurons, outputOfHiddenNeurons, learningRate,momentum) # backpropage and adjust weights\n",
    "\n",
    "        #-----------------------Calculate the mean squared error\n",
    "        totalError = 0\n",
    "        for x in range(0,26):\n",
    "            squared     = 0.5 * outputError[x]**2\n",
    "            totalError  = totalError + squared\n",
    "            \n",
    "        print('Total Error = %f' %totalError)\n",
    "        iteration = iteration + 1\n",
    "        errorList.append(totalError)\n",
    "        y_axis.append(iteration)\n",
    "        \n",
    "    #------------------Plot Total Error vs Iteration\n",
    "    print('Total Number of iterations %d' %iteration)\n",
    "    plt.plot(y_axis, errorList)\n",
    "    plt.ylabel('Total Error')\n",
    "    plt.xlabel('Number Of Iterations')\n",
    "    plt.show()\n",
    "    \n",
    "    return (Wi_h, Wh_o, Bh, Bo)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "#------------------------------Recognize Character---------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------               \n",
    "    \n",
    "def recognizeCharacter(inputNormalized,Wi_h,Wh_o,Bh,Bo):                # Returns the character recognized \n",
    "   \n",
    "    outputOfOutNeurons, outputOfHiddenNeurons = feedForward(inputNormalized, Wi_h, Wh_o, Bh, Bo) # feed forward\n",
    "    maxOut                                    = np.argmax(outputOfOutNeurons)                    # character recognized is neuron with highest output\n",
    "    return letters[maxOut]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
